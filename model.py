import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
import os
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.model_selection import cross_val_score
import pickle

# launch data file loading process
os.system('data_file_load.py')

## Load the training and validation data in to lists

training_data_array = []
validation_data_array = []
malware_data_array = []
bf = 1

filename = "ADFA-LD/training_data.csv"
# filename = "B:\\Programming\\Python\\2431\\Malware_Detection_SysCallSeq\\ADFA-LD\\training_data.csv"
with open(filename, 'r') as fin:
    for line in fin:
        training_data_array.append(str(line))

filename = "ADFA-LD/validation_data.csv"
# filename = "B:\\Programming\\Python\\2431\\Malware_Detection_SysCallSeq\\ADFA-LD\\validation_data.csv"
with open(filename, 'r') as fin:
    for line in fin:
        validation_data_array.append(str(line))

filename = "ADFA-LD/malware_data.csv"
# filename = "B:\\Programming\\Python\\2431\\Malware_Detection_SysCallSeq\\ADFA-LD\\malware_data.csv"
with open(filename, 'r') as fin:
    for line in fin:
        malware_data_array.append(str(line))

# Removes the junk first line
validation_data_array = validation_data_array[1:]

malware_data_array = np.array(malware_data_array)
validation_data_array = np.array(validation_data_array)
training_data_array = np.array(training_data_array)

zeros = np.zeros(training_data_array.shape[0])
training_data_array = np.vstack((training_data_array, zeros)).T

zeros = np.zeros(validation_data_array.shape[0])
validation_data_array = np.vstack((validation_data_array, zeros)).T

ones = np.ones(malware_data_array.shape[0])
malware_data_array = np.vstack((malware_data_array, ones)).T

length_all_data = 5950

temp_data = np.concatenate((training_data_array, validation_data_array))
all_data = np.concatenate((temp_data, malware_data_array))

labels = ['Sequence', 'Malware']
df = pd.DataFrame(all_data)
df.columns = labels
df.head(10)

## Get max number of system calls

all_seqs = np.array(df['Sequence'])
max_calls = 0
for line in all_seqs:
    x = list(map(int, line.split()))
    # max number of calls in a single sequence
    if len(x) > max_calls:
        max_calls = len(x)

call_dataframe = pd.DataFrame(index=range(length_all_data), columns=range(max_calls))

## Split in to 3 seq cells

r = 0
for line in all_seqs:
    c = 0
    # print(line)
    x = list(map(int, line.split()))

    for i in range(len(x) - 3):
        arr = ' '.join(map(str, x[i:i + 3]))
        call_dataframe.at[r, c] = arr
        c += 1
    r += 1

# Replace NaN with -1
call_dataframe.fillna(str(-1), inplace=True)
# Add the malware column, target
target_strings = []
for entry in df['Malware']:
    p = str(entry)
    target_strings.append(p)
call_dataframe['Malware'] = pd.Series(target_strings, index=call_dataframe.index)
call_dataframe.head()

target = np.array(df['Malware'])
temp = target.astype(float)
labels = temp.astype(int)

le = LabelEncoder()
onehot_df = call_dataframe.apply(le.fit_transform)
unique_calls = pd.unique(onehot_df[:].values.ravel('K'))
unique_calls.sort()
true_k = len(np.unique(unique_calls))
onehot_df.head(3)

bag_of_words_t_arr = onehot_df.drop(['Malware'], axis=1).to_numpy()
bag_of_words_arr = np.array([str(row) for row in bag_of_words_t_arr])

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(bag_of_words_arr)
print("n_samples: %d, n_features: %d" % X.shape)

doc_ind = 10  # Index of an example document
xi = X[doc_ind, :].todense()
term_ind = xi.argsort()[:, ::-1]
xi_sort = xi[0, term_ind]
terms = vectorizer.get_feature_names()

for i in range(10):
    term = terms[term_ind[0, i]]
    tfidf = xi[0, term_ind[0, i]]
    print('{0:20s} {1:f} '.format(term, tfidf))

km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,
            verbose=True, random_state=1)
km.fit(X)

order_centroids = km.cluster_centers_.argsort()[:, ::-1]
for i in range(10):
    print("Cluster %d:" % i, end='')
    for ind in order_centroids[i, :10]:
        print(' %s' % terms[ind], end='')
    print()

labelkm = km.labels_
C = confusion_matrix(labels, labelkm)
Csum = np.sum(C, axis=0)
Cnorm = C / Csum[None, :]
with np.printoptions(precision=3, suppress=True):
    print(Cnorm)

Xtr, Xts, Ytr, Yts = train_test_split(X, labels)

model = MultinomialNB()
model.fit(Xtr, Ytr)
y_pred = model.predict(Xts)
print(confusion_matrix(Yts, y_pred))
print(classification_report(Yts, y_pred, digits=4))
print("Accuracy:", accuracy_score(Yts, y_pred))

## Flatten datasets

call_df = pd.DataFrame(bag_of_words_arr)
call_df['label'] = labels
call_df = call_df.rename(columns={0: "Seq"})
call_df.head(3)

vectorizer = CountVectorizer()
count = vectorizer.fit_transform(bag_of_words_arr)
Xtr, Xts, Ytr, Yts = train_test_split(count, labels)
model2 = MultinomialNB()
model2.fit(Xtr, Ytr)
y_pred = model2.predict(Xts)
print(confusion_matrix(Yts, y_pred))
print(classification_report(Yts, y_pred, digits=4))
print("Accuracy:", accuracy_score(Yts, y_pred))

## Without OHEC

tfidf2 = TfidfVectorizer(
    sublinear_tf=True,
    min_df=2,
    norm='l2',
    ngram_range=(3, 5)
)
features2 = tfidf2.fit_transform(df.Sequence).toarray()
labels2 = df.Malware
print(features2.shape)
df.head(10)

X_train, X_test, y_train, y_test = train_test_split(df['Sequence'], df['Malware'], random_state=0)
count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(X_train)
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
clf = MultinomialNB().fit(X_train_tfidf, y_train)

y_pred = clf.predict(count_vect.transform(X_test))
print(classification_report(y_test, y_pred, digits=4))
print("Accuracy:", accuracy_score(y_test, y_pred))

print(clf.predict(count_vect.transform(["6 6 64 45 33 29 125 125 125"])))

model = LinearSVC()
entries = []
CV = 5
model_name = model.__class__.__name__
accuracies = cross_val_score(model, features2, labels2, scoring='accuracy', cv=CV)
for fold_idx, accuracy in enumerate(accuracies):
    entries.append((model_name, fold_idx, accuracy))
cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])

cv_df.groupby('model_name').accuracy.mean()

X_train, X_test, y_train, y_test = train_test_split(features2, labels2, test_size=0.33, random_state=0)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred, digits=4))
print("Accuracy:", accuracy_score(y_test, y_pred))

pickle.dump(count_vect, open("countvec.pickle.dat", "wb"))
pickle.dump(clf, open("model.pickle.dat", "wb"))
