import pandas as pd
import numpy as np
import sys
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
import xgboost as xgb
from sklearn.metrics import accuracy_score
import pickle
from sklearn import linear_model, preprocessing

# Load the training and validation data in to lists

training_data_array = []
validation_data_array = []
malware_data_array = []

filename = "ADFA-LD/training_data.csv"
# filename = "B:\\Programming\\Python\\2431\\Malware_Detection_SysCallSeq\\ADFA-LD\\training_data.csv"
with open(filename, 'r') as fin:
    for line in fin:
        training_data_array.append(str(line))

filename = "ADFA-LD/validation_data.csv"
# filename = "B:\\Programming\\Python\\2431\\Malware_Detection_SysCallSeq\\ADFA-LD\\validation_data.csv"
with open(filename, 'r') as fin:
    for line in fin:
        validation_data_array.append(str(line))

filename = "ADFA-LD/malware_data.csv"
# filename = "B:\\Programming\\Python\\2431\\Malware_Detection_SysCallSeq\\ADFA-LD\\malware_data.csv"
with open(filename, 'r') as fin:
    for line in fin:
        malware_data_array.append(str(line))

# Removes the junk first line
validation_data_array = validation_data_array[1:]

malware_data_array = np.array(malware_data_array)
validation_data_array = np.array(validation_data_array)
training_data_array = np.array(training_data_array)

zeros = np.zeros(training_data_array.shape[0])
training_data_array = np.vstack((training_data_array, zeros)).T

zeros = np.zeros(validation_data_array.shape[0])
validation_data_array = np.vstack((validation_data_array, zeros)).T

ones = np.ones(malware_data_array.shape[0])
malware_data_array = np.vstack((malware_data_array, ones)).T

length_all_data = 5950


temp_data = np.concatenate((training_data_array, validation_data_array))
all_data = np.concatenate((temp_data, malware_data_array))

# extract 3 system call sequences, put in to array

labels = ['Sequence', 'Malware']
df = pd.DataFrame(all_data)
df.columns = labels

# Get max number of system calls

all_seqs = np.array(df['Sequence'])
max_calls = 0
for line in all_seqs:
    x = list(map(int, line.split()))
    # max number of calls in a single sequence
    if len(x) > max_calls:
        max_calls = len(x)

call_dataframe = pd.DataFrame(index=range(length_all_data), columns=range(max_calls))

# Split in to 3 seq cells

r = 0
for line in all_seqs:
    c = 0
    # print(line)
    x = list(map(int, line.split()))

    for i in range(len(x) - 3):
        arr = ' '.join(map(str, x[i:i + 3]))
        call_dataframe.at[r, c] = arr
        c += 1
    r += 1

labels = []
for i in range(0, max_calls):
    labels.append(str(i))
call_dataframe.columns = labels

# Replace NaN with -1
call_dataframe.fillna(str(-1), inplace=True)
# Add the malware column, target
target_strings = []
for entry in df['Malware']:
    p = str(entry)
    target_strings.append(p)
call_dataframe['Malware'] = pd.Series(target_strings, index=call_dataframe.index)

# One hot encoding

le = LabelEncoder()
onehot_df = call_dataframe.apply(le.fit_transform)

# Make list of all unique words in the onehot_df

unique_labels = pd.unique(onehot_df[labels].values.ravel('K'))
unique_labels.sort()

# New Dataframe for unique system call sequences

call_dataframe_unique = pd.DataFrame(index=range(length_all_data), columns=range(unique_labels.shape[0]))
call_dataframe_unique.columns = unique_labels

# Split dataframe into df_test and df_train 20-80 split

split = np.random.rand(len(onehot_df)) < .8
df_train = onehot_df[split]
df_test = onehot_df[~split]

# Split dataframes into X_train, Y_train, X_Test, Y_Test

df_train_target = df_train['Malware']
df_test_target = df_test['Malware']

df_train_features = df_train.loc[:, df_train.columns != 'Malware']
df_test_features = df_test.loc[:, df_test.columns != 'Malware']

# Dataframe to dmatrix
dm_train = xgb.DMatrix(data=df_train_features.values, label=df_train_target.values)
dm_test = xgb.DMatrix(data=df_test_features.values)

model = xgb.XGBClassifier(objective='reg:linear', colsample_bytree=0.3, learning_rate=0.1,
                          max_depth=5, alpha=10, n_estimators=10)
model.fit(df_train_features.values, df_train_target.values)
y_pred = model.predict(df_test_features.values)
predictions = [round(value) for value in y_pred]
acc = accuracy_score(df_test_target, predictions)
print("Accuracy: ", acc)

pickle.dump(model, open("model.pickle.dat", "wb"))
print("Finished model")
