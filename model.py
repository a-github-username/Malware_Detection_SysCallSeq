# %%

import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
import xgboost as xgb
from sklearn.metrics import accuracy_score
import pickle
from sklearn import linear_model, preprocessing
import sys

## Load the training and validation data in to lists

training_data_array = []
validation_data_array = []
malware_data_array = []

filename = "ADFA-LD/training_data.csv"
# filename = "B:\\Programming\\Python\\2431\\Malware_Detection_SysCallSeq\\ADFA-LD\\training_data.csv"
with open(filename, 'r') as fin:
    for line in fin:
        training_data_array.append(str(line))

filename = "ADFA-LD/validation_data.csv"
# filename = "B:\\Programming\\Python\\2431\\Malware_Detection_SysCallSeq\\ADFA-LD\\validation_data.csv"
with open(filename, 'r') as fin:
    for line in fin:
        validation_data_array.append(str(line))

filename = "ADFA-LD/malware_data.csv"
# filename = "B:\\Programming\\Python\\2431\\Malware_Detection_SysCallSeq\\ADFA-LD\\malware_data.csv"
with open(filename, 'r') as fin:
    for line in fin:
        malware_data_array.append(str(line))

# Removes the junk first line
validation_data_array = validation_data_array[1:]

malware_data_array = np.array(malware_data_array)
validation_data_array = np.array(validation_data_array)
training_data_array = np.array(training_data_array)

zeros = np.zeros(training_data_array.shape[0])
training_data_array = np.vstack((training_data_array, zeros)).T

zeros = np.zeros(validation_data_array.shape[0])
validation_data_array = np.vstack((validation_data_array, zeros)).T

ones = np.ones(malware_data_array.shape[0])
malware_data_array = np.vstack((malware_data_array, ones)).T

length_all_data = 5950

temp_data = np.concatenate((training_data_array, validation_data_array))
all_data = np.concatenate((temp_data, malware_data_array))

# extract 3 system call sequences, put in to array

labels = ['Sequence', 'Malware']
df = pd.DataFrame(all_data)
df.columns = labels

# Get max number of system calls

all_seqs = np.array(df['Sequence'])
max_calls = 0
for line in all_seqs:
    x = list(map(int, line.split()))
    # max number of calls in a single sequence
    if len(x) > max_calls:
        max_calls = len(x)

call_dataframe = pd.DataFrame(index=range(length_all_data), columns=range(max_calls))

# Split in to 3 seq cells

r = 0
for line in all_seqs:
    c = 0
    x = list(map(int, line.split()))

    for i in range(len(x) - 3):
        arr = ' '.join(map(str, x[i:i + 3]))
        call_dataframe.at[r, c] = arr
        c += 1
    r += 1


labels = []
for i in range(0, max_calls):
    labels.append(str(i))
call_dataframe.columns = labels


# Replace NaN with -1
call_dataframe.fillna(str(-1), inplace=True)
# Add the malware column, target
target_strings = []
for entry in df['Malware']:
    p = str(entry)
    target_strings.append(p)
call_dataframe['Malware'] = pd.Series(target_strings, index=call_dataframe.index)

# One hot encoding

le = LabelEncoder()
onehot_df = call_dataframe.apply(le.fit_transform)

# Make list of all unique words in the onehot_df

unique_labels = pd.unique(onehot_df[labels].values.ravel('K'))
unique_labels.sort()

# Number of unique calls per line

call_dataframe['Malware'] = call_dataframe['Malware'].astype(float)
call_dataframe['Malware'] = call_dataframe['Malware'].astype(int)
split_df = ((onehot_df.apply(pd.value_counts, axis=1).fillna(0)).drop(0, axis=1)).join(call_dataframe['Malware'])

# Split dataframe into df_test and df_train 20-80 split

split = np.random.rand(len(split_df)) < .8
df_train = split_df[split]
df_test = split_df[~split]

# %%

# Split dataframes into X_train, Y_train, X_Test, Y_Test

df_train_target = df_train['Malware']
df_test_target = df_test['Malware']

df_train_features = df_train.loc[:, df_train.columns != 'Malware']
df_test_features = df_test.loc[:, df_test.columns != 'Malware']
# df_train_features.drop(labels, axis=1)
# df_test_features.drop(labels, axis=1)

# Import ML Packages, set xbg DMatrices

# Dataframe to dmatrix
dm_train = xgb.DMatrix(data=df_train_features.values, label=df_train_target.values)
dm_test = xgb.DMatrix(data=df_test_features.values)

model = xgb.XGBClassifier(objective='reg:linear', colsample_bytree=0.3, learning_rate=0.1,
                          max_depth=5, alpha=10, n_estimators=10)
model.fit(df_train_features.values, df_train_target.values)
y_pred = model.predict(df_test_features.values)
predictions = [round(value) for value in y_pred]
acc = accuracy_score(df_test_target, predictions)
print("Accuracy: ", acc)

# xgb.plot_importance(model, importance_type='cover')
# model.get_booster().get_score(importance_type='gain')
sorted_idx = np.argsort(model.feature_importances_)[::-1]

xgb.plot_importance(model, max_num_features=20)
plt.show()


all_vars = df_train_features.columns.tolist()
best_features = [1, 10, 2, 6, 1045, 1010, 39, 304, 169, 1053, 261, 14, 924, 407, 249, 1005, 986, 206, 203, 943]
best_features.sort()
worst_vars = [cols for cols in all_vars if cols not in best_features]

x_train = df_train_features.drop(worst_vars, axis=1)
x_test = df_test_features.drop(worst_vars, axis=1)

logit_model = linear_model.LogisticRegression()
# Fit
logit_model = logit_model.fit(x_train.values, df_train_target.values)
# How accurate?
logit_model.score(x_test.values, df_test_target.values)

pickle.dump(logit_model, open("model.pickle.dat", "wb"))
print("Model finished")
sys.exit(0)